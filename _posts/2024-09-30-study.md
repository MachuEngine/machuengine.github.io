---
layout: "single"
title: "[패턴인식] 베이시안 결정 이론"
categories:
  - pattern recognition
tags:
  - 패턴인식
---

## 보편적인 인식 법칙
가장 그럴듯한 부류로 분류할 수 있으며, 이는 확률로 표현할 수 있다.  
P(W|X) : 사후 확률(Posterior probability)  
(X가 주어졌을 때 그것이 부류 W에서 발생했을 확률)

<img src="/assets/images/bayesianRule (1).png" alt="패턴인식 - 확률로 인식을 표현">

---

## 랜덤 변수
여기서 W와 X는 랜덤 변수(Ramdom variable)이다.  
W = {w_1, w_2, ...}  
X = {x_1, x_2, ...}

<span style="font-size: smaller;">연속적인 경우에는 확률 밀도 함수(PDF, Probability density function)를 통해 표현된다.</span>
p(X)

<img src="/assets/images/bayesianRule (2).png" alt="패턴 인식 - PDF">

---

## 사후 확률
P(W|X) : 사후 확률(Posterior probability)  
(X가 주어졌을 때 그것이 부류 W에서 발생했을 확률)

---

## 사전 확률
P(X) : 사전 확률(Prior probability)  
(X가 주어질 확률)

---

## 우도 (Likelihood)
P(X|W) : 우도(Likelihood)  
(부류 W가 선택되었을 때 X가 선택될 확률, 즉 조건부 확률)

---

## 문제 해결을 위해 어떤 확률을 사용해야 하는가?
우도와 사전 확률을 모두 고려해야 한다.  
사후 확률은 우도와 사전 확률을 통해 표현된다.  
따라서, 사후 확률을 사용하여 분류하는 것이 성능이 더 나을 수 있다.

---

## 베이스 정리 (Bayes Rule)

<img src="/assets/images/bayesianRule (3).png" alt="패턴 인식 - 베이스 정리">

---

## Likelihood, Posterior

**Likelihood**  
<img src="/assets/images/bayesianRule (4).png" alt="패턴 인식 - Likelihood">

**Posterior**  
<img src="/assets/images/bayesianRule (5).png" alt="패턴 인식 - Posterior">

w1과 w2의 두 분류의 확률 밀도 함수의 총합은 1이 되어야 한다.  
두 확률 밀도 함수가 겹치는 부분은 오류 사례(Error Case)이다.

---

## 평균과 분산 (평균 벡터와 공분산 행렬)

<img src="/assets/images/bayesianRule (6).png" alt="패턴 인식 - 평균벡터 공분산 행렬 공식">

μ : sample mean (평균)  
Σ : sample variance (공분산)

---

## 평균 벡터와 공분산 행렬의 예시

<img src="/assets/images/bayesianRule (7).png" alt="패턴 인식 - 평균벡터 공분산 행렬 예시">

---

## 베이시안 분류기 (Bayesian Classifier)

분류기 학습(Training)에 사용되는 집합  
training set X = {(x_1, t_1), (x_2, t_2), ... , (x_N, t_N)}  
X = (x_1, x_2, ..., X_d)는 feature vector이다.  
t_i는 w_1, w_2, ..., w_M에 속한 부류 중 하나이다. (이진 분류기인 경우 M=2)

<img src="/assets/images/bayesianRule (8).png" alt="패턴 인식 - 필기 숫자 예시">

---

## 최소 오류 베이시안 분류기 (Optimal Bayesian Classifier)

주어진 특징 벡터 X에 대해 '가장 그럴듯한' 부류로 분류한다.  
"P(w_1|X) > P(w_2|X) 이면, X를 w_1로 분류" -> 사후 확률을 기반으로 판단한다.  
베이스 정리를 이용해 사후 확률을 구할 수 있다.

<img src="/assets/images/bayesianRule (9).png" alt="패턴 인식 - 사후 확률 공식, 베이시안 룰">  
(여기서 분모는 무시할 수 있다.)

<img src="/assets/images/bayesianRule (10).png" alt="패턴 인식 - 사전 확률과 우도 계산">

<img src="/assets/images/bayesianRule (11).png" alt="패턴 인식 - 최소 오류 그래프">

<img src="/assets/images/bayesianRule (12).png" alt="패턴 인식 - 최소 오류가 아닌 그래프">

---

## 최소 위험 베이시안 분류기
### 손실 행렬 
loss matrix 또는 loss function (loss는 risk, cost 등으로 불릴 수 있다.)  
c_ij : true class i, decision j (i=j인 경우는 올바른 예측을 한 경우)  
<img src="/assets/images/bayesianRule (13).png" alt="패턴 인식 - 손실 행렬">

### 우도비 T
손실 함수가 적용된 상태에서 우도비를 통해 decision할 수 있다. 
우도비 T
<img src="/assets/images/bayesianRule (14).png" alt="패턴 인식 - 우도비1">

유도 과정
<img src="/assets/images/bayesianRule (15).png" alt="패턴 인식 - 우도비2">

손실값에 따라 Decision point가 달라진다. 그리고 우도비 T의 크기에
따라서 R_1 / R_2의 영역이 변한다.  
T가 커지면 p(x|w_1)이 p(x|w_2)보다 많이 커야하므로, T보다 큰 경우의 수가 줄어든다.  
그러므로 R_1의 영역이 줄어든다.  
<img src="/assets/images/bayesianRule (16).png" alt="패턴 인식 - 우도비3">

## M부류로 확장 
2-클래스 분류 문제에서 M-클래스 분류 문제로 확장되는 경우의 공식은 아래와 같이 정리된다. 
<img src="/assets/images/bayesianRule (17).png" alt="패턴 인식 - M부류">

### 분별함수
위 공식 중 3번째 분별 함수 gi(x)는 분별함수이다.  
분별 함수 gi(x)가 최대가 되는 i를 찾고, 그 i를 k라고 하자. 
즉, d차원의 데이터 X가 i번째 분별함수에 입력되었을 때, 값이 최대가 되는 경우 그 i부류로 분류된다.  

네트워크 형태로 표현한 베이지안 분류기
<img src="/assets/images/bayesianRule (18).png" alt="패턴 인식 - M부류">

